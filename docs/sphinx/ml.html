<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN"
  "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">


<html xmlns="http://www.w3.org/1999/xhtml">
  <head>
    <meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
    
    <title>Machine Learning &mdash; libSkylark  documentation</title>
    
    <link rel="stylesheet" href="_static/haiku.css" type="text/css" />
    <link rel="stylesheet" href="_static/pygments.css" type="text/css" />
    <link rel="stylesheet" href="_static/print.css" type="text/css" />
    
    <script type="text/javascript">
      var DOCUMENTATION_OPTIONS = {
        URL_ROOT:    './',
        VERSION:     '',
        COLLAPSE_INDEX: false,
        FILE_SUFFIX: '.html',
        HAS_SOURCE:  true
      };
    </script>
    <script type="text/javascript" src="_static/jquery.js"></script>
    <script type="text/javascript" src="_static/underscore.js"></script>
    <script type="text/javascript" src="_static/doctools.js"></script>
    <script type="text/javascript" src="_static/theme_extras.js"></script>
    <link rel="top" title="libSkylark  documentation" href="index.html" />
    <link rel="next" title="IO in libSkylark" href="io.html" />
    <link rel="prev" title="Numerical Linear Algebra Primitives" href="nla.html" /> 
  </head>
  <body>
      <div class="header"><h1 class="heading"><a href="index.html">
          <span>libSkylark  documentation</span></a></h1>
        <h2 class="heading"><span>Machine Learning</span></h2>
      </div>
      <div class="topnav">
      
        <p>
        «&#160;&#160;<a href="nla.html">Numerical Linear Algebra Primitives</a>
        &#160;&#160;::&#160;&#160;
        <a class="uplink" href="index.html">Contents</a>
        &#160;&#160;::&#160;&#160;
        <a href="io.html">IO in libSkylark</a>&#160;&#160;»
        </p>

      </div>
      <div class="content">
        
        
  <div class="section" id="machine-learning">
<h1>Machine Learning<a class="headerlink" href="#machine-learning" title="Permalink to this headline">¶</a></h1>
<div class="section" id="randomized-kernel-methods">
<h2>Randomized Kernel Methods<a class="headerlink" href="#randomized-kernel-methods" title="Permalink to this headline">¶</a></h2>
<p>libSkylark provides distributed implementations of kernel-based nonlinear models for</p>
<blockquote>
<div><ul class="simple">
<li>Regularized Least Squares Regression and Classification</li>
<li>Regularized Robust Regression (Least Absolute Deviation loss)</li>
<li>Support Vector Machines</li>
<li>Multinomial Logistic Regression (classes &gt; 2).</li>
</ul>
</div></blockquote>
<p>The following kernels are supported:</p>
<blockquote>
<div><ul class="simple">
<li>Gaussian and Laplacian Kernels via Random Fourier Transform (<a class="reference external" href="http://www.eecs.berkeley.edu/~brecht/papers/07.rah.rec.nips.pdf">Rahimi and Recht, 2007</a>)</li>
<li>Gaussian and Laplacian Kernels via Fast Random Fourier Transform (<a class="reference external" href="http://jmlr.org/proceedings/papers/v28/le13.html">Le, Sarlos and Smola, 2013</a>)</li>
<li>Polynomial Kernels via Tensor Sketch (<a class="reference external" href="http://www.itu.dk/people/ndap/TensorSketch.pdf">Pahm and Pagh, 2013</a>)</li>
<li>Exponential Semigroup Kernels via Random Laplace Transform (<a class="reference external" href="http://vikas.sindhwani.org/RandomLaplace.pdf">Yang et al, 2014</a>)</li>
</ul>
</div></blockquote>
<dl class="docutils">
<dt>The implementations combine two ideas:</dt>
<dd><ul class="first last simple">
<li>Constructing randomized approximations to Kernel functions <em>on the fly</em></li>
<li>Using a distributed optimization solver based on Alternating Directions Method of Multipliers (ADMM)</li>
</ul>
</dd>
</dl>
<p>The distributed optimization approach is based on a block-splitting variant of ADMM proposed in <a class="reference external" href="http://web.stanford.edu/~boyd/papers/block_splitting.html">Parikh and Boyd, 2014</a></p>
<dl class="docutils">
<dt>The full implementation (under <tt class="docutils literal"><span class="pre">libskylark/ml</span></tt>) is described in the following paper:</dt>
<dd><ul class="first last simple">
<li>Sindhwani V. and Avron H., High-performance Kernel Machines with Implicit Distributed Optimization and Randomization, 2014</li>
</ul>
</dd>
</dl>
<div class="section" id="standalone-usage">
<h3>Standalone Usage<a class="headerlink" href="#standalone-usage" title="Permalink to this headline">¶</a></h3>
<p>Building libSkylark creates an executable called <strong>skylark_ml</strong> under CMAKE_PREFIX_INSTALL/bin. This executable can be
used out-of-the-box for large-scale applications involving kernel-based modeling.</p>
</div>
<div class="section" id="input-data-format">
<span id="ml-io"></span><h3>Input Data Format<a class="headerlink" href="#input-data-format" title="Permalink to this headline">¶</a></h3>
<p>The implementation supports <a class="reference external" href="http://www.csie.ntu.edu.tw/~cjlin/libsvmtools/datasets/">LIBSVM</a> file format, where
feature vectors and labels are specified as</p>
<div class="highlight-python"><pre>0 1:0.2 4:0.5 10:0.3
5 3:0.3 6:0.1</pre>
</div>
<p>Each line begins with a label and followed by index (starting with 1)-value pairs describing the feature vector in
sparse format.</p>
<p>We also support <a class="reference external" href="http://www.hdfgroup.org/HDF5/">HDF5</a> data files.</p>
<blockquote>
<div><ul>
<li><dl class="first docutils">
<dt>Dense training data can be described using HDF5 files containing two <a class="reference external" href="http://www.hdfgroup.org/HDF5/Tutor/crtdat.html">HDF5 datasets</a>:</dt>
<dd><ul class="first last simple">
<li><em>X</em> &#8211; n x d matrix  (examples-by-features)</li>
<li><em>Y</em> &#8211; n x 1 matrix of labels.</li>
</ul>
</dd>
</dl>
</li>
<li><dl class="first docutils">
<dt>Sparse training data can be described using HDF5 files containing five <a class="reference external" href="http://www.hdfgroup.org/HDF5/Tutor/crtdat.html">HDF5 datasets</a> specifying a Compressed Row Storage Sparse matrix:</dt>
<dd><ul class="first last simple">
<li><em>dimensions</em>: 3 x 1 matrix [number of features, number of examples, number of nonzeros (nnz)]</li>
<li><em>indices</em>: nnz x 1 matrix column indices of non-zero values for CRS datastructure representing the examples-by-features sparse matrix</li>
<li><em>values</em>: nnz x 1 non-zero values corresponding to indices</li>
<li><em>indptr</em>: (n+1) x 1 - pointer into indices, values specifying rows</li>
<li><em>Y</em>: n x 1 matrix of labels</li>
</ul>
</dd>
</dl>
</li>
</ul>
</div></blockquote>
<p>Examples of such files can be downloaded from <a class="reference external" href="http://vikas.sindhwani.org/data.tar.gz">here</a>. The HDF5 files can be viewed using <a class="reference external" href="http://http://www.hdfgroup.org/HDF5/Tutor/hdfview.html">HDFView</a>. A screenshot is shown below.</p>
<a class="reference internal image-reference" href="_images/hdfview_screenshot.png"><img alt="_images/hdfview_screenshot.png" class="align-center" src="_images/hdfview_screenshot.png" style="width: 750px;" /></a>
<div class="admonition note">
<p class="first admonition-title">Note</p>
<p class="last">For all fileformats described above, the current implementation is geared towards classification problems and requires the label to assume values from 0 to (K-1) for a K-class problem, or +1/-1 for binary classification problems. This assumption will be relaxed.</p>
</div>
</div>
</div>
<div class="section" id="example-and-commandline-usage">
<h2>Example and Commandline Usage<a class="headerlink" href="#example-and-commandline-usage" title="Permalink to this headline">¶</a></h2>
<p>Please see <a class="reference internal" href="quick_start.html#ml-example"><em>ML</em></a></p>
<div class="highlight-python"><pre>Training mode usage: skylark_ml [options] --trainfile trainfile --modelfile modelfile
Testing mode usage: skylark_ml --modelfile modelfile --testfile testfile
  -h [ --help ]                         produce a help message
  -l [ --lossfunction ] arg (=0)        Loss function (0:SQUARED (L2), 1:LAD
                                        (L1), 2:HINGE, 3:LOGISTIC)
  -r [ --regularizer ] arg (=0)         Regularizer (0:L2, 1:L1)
  -k [ --kernel ] arg (=0)              Kernel (1:GAUSSIAN,
                                        2:POLYNOMIAL, 3:LAPLACIAN,
                                        4:EXPSEMIGROUP)
  -g [ --kernelparam ] arg (=1)         Kernel Parameter
  -x [ --kernelparam2 ] arg (=0)        If Applicable - Second Kernel Parameter
                                        (Polynomial Kernel: c)
  -y [ --kernelparam3 ] arg (=1)        If Applicable - Third Kernel Parameter
                                        (Polynomial Kernel: gamma)
  -c [ --lambda ] arg (=0)              Regularization Parameter
  -e [ --tolerance ] arg (=0.001)       Tolerance
  --rho arg (=1)                        ADMM rho parameter
  -s [ --seed ] arg (=12345)            Seed for Random Number Generator
  -f [ --randomfeatures ] arg (=100)    Number of Random Features (default:
                                        100)
  -n [ --numfeaturepartitions ] arg (=1)
                                        Number of Feature Partitions (default:
                                        1)
  -t [ --numthreads ] arg (=1)          Number of Threads (default: 1)
  --regular arg (=1)                    Default is to use 'fast' feature
                                        mapping, if available.Use this flag to
                                        force regular mapping (default: false)
  --cachetransforms arg (=0)            Default is to not cache feature
                                        transforms per iteration, but generate
                                        on fly. Use this flag to force
                                        transform caching if you have enough
                                        memory (default: false)
  --fileformat arg (=0)                 Fileformat (default: 0 (libsvm-&gt;dense),
                                        1 (libsvm-&gt;sparse), 2 (hdf5-&gt;dense), 3
                                        (hdf5-&gt;sparse)
  -i [ --MAXITER ] arg (=100)           Maximum Number of Iterations (default:
                                        100)
  --trainfile arg                       Training data file (required in
                                        training mode)
  --modelfile arg                       Model output file
  --valfile arg                         Validation file (optional)
  --testfile arg                        Test file (optional in training mode;
                                        required in testing mode)</pre>
</div>
</div>
<div class="section" id="library-usage">
<h2>Library Usage<a class="headerlink" href="#library-usage" title="Permalink to this headline">¶</a></h2>
<p>To be documented (please see <tt class="docutils literal"><span class="pre">ml/run.hpp</span></tt> for a driver program).</p>
</div>
</div>


      </div>
      <div class="bottomnav">
      
        <p>
        «&#160;&#160;<a href="nla.html">Numerical Linear Algebra Primitives</a>
        &#160;&#160;::&#160;&#160;
        <a class="uplink" href="index.html">Contents</a>
        &#160;&#160;::&#160;&#160;
        <a href="io.html">IO in libSkylark</a>&#160;&#160;»
        </p>

      </div>

    <div class="footer">
        &copy; Copyright IBM Corporation 2012-2014.  All Rights Reserved.
      Created using <a href="http://sphinx-doc.org/">Sphinx</a> 1.2b1.
    </div>
  </body>
</html>